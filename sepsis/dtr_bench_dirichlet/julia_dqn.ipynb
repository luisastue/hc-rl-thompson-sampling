{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics\n",
    "using Gen\n",
    "using PyCall\n",
    "using Random\n",
    "gym = pyimport(\"gymnasium\")\n",
    "DTRGym = pyimport(\"DTRGym\")\n",
    "spaces = pyimport(\"gym.spaces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dist function labeled_categorical(labels, probs)\n",
    "    index = categorical(probs)\n",
    "    labels[index]\n",
    "end;\n",
    "\n",
    "const Action = Int\n",
    "struct State\n",
    "    hr::Int\n",
    "    bp::Int\n",
    "    o2::Int\n",
    "    glu::Float64\n",
    "    diabetic::Bool\n",
    "    abx::Bool\n",
    "    vaso::Bool\n",
    "    vent::Bool\n",
    "end;\n",
    "const Policy = Dict{State,Action}\n",
    "struct Episode\n",
    "    policy::Policy\n",
    "    rewards::Vector{Float64}\n",
    "    visited::Vector{State}\n",
    "end;\n",
    "\n",
    "const DirichletCounts = Dict{Tuple{State,Action,State},Int}\n",
    "ACTIONS = [i for i in 1:8]\n",
    "STATES = [State(hr, bp, o2, glu / 2, diabetic, abx, vaso, vent) for hr in -1:1 for bp in -1:1 for o2 in -1:1 for glu in -2:2 for diabetic in [true, false] for abx in [true, false] for vaso in [true, false] for vent in [true, false]]\n",
    "\n",
    "state_to_index = Dict(state => i for (i, state) in enumerate(STATES))\n",
    "\n",
    "function get_reward(state::State)::Float64\n",
    "    reward = 0.0\n",
    "    critical_counts = count(c -> c != 0, [state.hr, state.bp, state.o2, state.glu])\n",
    "    if critical_counts >= 3\n",
    "        reward = -1.0\n",
    "    elseif critical_counts == 0 && !state.abx && !state.vaso && !state.vent\n",
    "        reward = 1.0\n",
    "    end\n",
    "    return reward\n",
    "end;\n",
    "\n",
    "function to_state(dict::Dict{Any,Any})::State\n",
    "    return State(dict[\"hr_state\"], dict[\"sysbp_state\"], dict[\"percoxyg_state\"], dict[\"glucose_state\"], dict[\"diabetic_idx\"], dict[\"antibiotic_state\"], dict[\"vaso_state\"], dict[\"vent_state\"])\n",
    "end;\n",
    "\n",
    "const TransitionModel = Dict{Tuple{State, Action}, Vector{Float64}}\n",
    "\n",
    "function random_policy()::Policy\n",
    "    policy = Dict{State,Action}()\n",
    "    for state in STATES\n",
    "        policy[state] = rand(ACTIONS)\n",
    "    end\n",
    "    return policy\n",
    "end;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function transition_model(dirichlet_counts::DirichletCounts=DirichletCounts())::TransitionModel\n",
    "    # beliefs are a mapping S,A -> S\n",
    "    beliefs = TransitionModel()\n",
    "    for state in STATES\n",
    "        for action in ACTIONS\n",
    "            beliefs[(state, action)] = {state => action} ~ dirichlet([haskey(dirichlet_counts, (state,action,new_state)) ? dirichlet_counts[(state, action, new_state)] : 1.0 for new_state in STATES])\n",
    "        end\n",
    "    end\n",
    "    return beliefs\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function simulate_episode(beliefs::TransitionModel, actions::Vector{Action}, start_state::State)\n",
    "    states = [start_state]\n",
    "    state = start_state\n",
    "    rewards = []\n",
    "    for (t, action) in enumerate(actions)\n",
    "        {t => :action} ~ labeled_categorical([action], [1])\n",
    "        new_state = {t => :new_state} ~ labeled_categorical(STATES, beliefs[(state, action)])\n",
    "        push!(states, new_state)\n",
    "        reward = {t => :reward} ~ labeled_categorical([get_reward(new_state)], [1])\n",
    "        push!(rewards, reward)\n",
    "        state = new_state\n",
    "    end\n",
    "    return states, rewards\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Episode(Dict{State, Int64}(State(-1, 0, 1, 1.0, true, false, false, false) => 2, State(-1, 1, 1, 1.0, false, false, false, false) => 5, State(1, -1, -1, 0.5, true, false, true, false) => 7, State(1, 0, 1, 1.0, false, false, true, true) => 6, State(0, -1, -1, 0.0, true, true, false, false) => 7, State(1, 0, -1, -0.5, false, false, false, false) => 2, State(-1, 1, -1, 1.0, true, true, false, true) => 6, State(-1, -1, 0, -0.5, false, false, true, true) => 2, State(-1, -1, 0, 0.0, false, false, true, false) => 4, State(0, -1, 1, -0.5, false, false, false, false) => 1…), [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0], State[State(1, 0, 1, 0.0, false, false, false, false), State(1, 0, 1, 0.0, false, false, true, true), State(1, 0, 1, 0.0, false, false, true, false), State(1, 0, 1, 0.0, false, true, false, true), State(1, 0, 1, 0.0, false, false, false, true), State(1, 1, 1, 0.0, false, false, true, false), State(0, 0, 1, 0.0, false, true, false, false), State(0, 1, 1, 0.0, false, false, true, true), State(0, 1, 1, 0.0, false, false, false, false), State(-1, 1, 1, 0.0, false, false, true, true), State(-1, 1, 1, 0.0, false, false, false, true), State(-1, 0, 1, -0.5, false, true, false, false), State(-1, 1, 1, -0.5, false, false, true, true)])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function run_episode(env, policy::Policy, max_length::Int)\n",
    "    obs, info = env.reset()\n",
    "    state = to_state(info[\"state\"])\n",
    "    visited = [state]\n",
    "    rewards = []\n",
    "    for t in 1:max_length\n",
    "        action = policy[state]\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        new_state = to_state(info[\"state\"])\n",
    "        push!(visited, new_state)\n",
    "        push!(rewards, reward)\n",
    "        state = new_state\n",
    "        if terminated\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    return Episode(policy, rewards, visited)\n",
    "end;\n",
    "\n",
    "sepsis_env = gym.make(\"OberstSepsisEnv-discrete\")\n",
    "run_episode(sepsis_env, random_policy(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Episode(Dict{State, Int64}(State(-1, 0, 1, 1.0, true, false, false, false) => 6, State(-1, 1, 1, 1.0, false, false, false, false) => 1, State(1, -1, -1, 0.5, true, false, true, false) => 1, State(1, 0, 1, 1.0, false, false, true, true) => 2, State(0, -1, -1, 0.0, true, true, false, false) => 4, State(1, 0, -1, -0.5, false, false, false, false) => 6, State(-1, 1, -1, 1.0, true, true, false, true) => 7, State(-1, -1, 0, -0.5, false, false, true, true) => 5, State(-1, -1, 0, 0.0, false, false, true, false) => 4, State(0, -1, 1, -0.5, false, false, false, false) => 8…), [-1.0], State[State(-1, 0, 0, 0.0, false, false, false, false), State(-1, 1, 0, 0.0, false, false, true, false)])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function run_episode_sepsis_model(env, model, max_length::Int)\n",
    "    obs, info = env.reset()\n",
    "    state = to_state(info[\"state\"])\n",
    "    visited = [state]\n",
    "    rewards = []\n",
    "    for t in 1:max_length\n",
    "        action = model.policy.predict(obs)[1][1]\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        new_state = to_state(info[\"state\"])\n",
    "        push!(visited, new_state)\n",
    "        push!(rewards, reward)\n",
    "        state = new_state\n",
    "        if terminated\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    return Episode(model, rewards, visited)\n",
    "end;\n",
    "\n",
    "sepsis_env = gym.make(\"OberstSepsisEnv-discrete\")\n",
    "run_episode(sepsis_env, random_policy(), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <module 'stable_baselines3' from '/Users/luisastue/miniconda3/lib/python3.10/site-packages/stable_baselines3/__init__.py'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import Stable-Baselines3\n",
    "sb3 = pyimport(\"stable_baselines3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PPO agent\n",
    "ppo = sb3.PPO(\"MlpPolicy\", sepsis_env, verbose=0)\n",
    "\n",
    "# Train the agent using `learn`\n",
    "ppo.learn(total_timesteps=100000, log_interval=1000) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_policy_sepsis_model(model)\n",
    "    policy = Dict{State, Int}()\n",
    "    for state in STATES\n",
    "        policy[state] = model.policy.predict([state.hr, state.bp, state.o2, state.abx, state.vaso, state.vent])[1][1]\n",
    "    end\n",
    "    return policy\n",
    "end;\n",
    "\n",
    "\n",
    "function get_policy(model)\n",
    "    policy = Dict{State, Int}()\n",
    "    for i in 1:length(STATES)\n",
    "        policy[STATES[i]] = model.policy.predict([i-1])[1][1]\n",
    "    end\n",
    "    return policy\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_episodes = []\n",
    "for _ in 1:100\n",
    "    ppo_sepsis_optimal_policy = get_policy_sepsis_model(ppo)\n",
    "    for _ in 1:1000\n",
    "        push!(eval_episodes, run_episode(sepsis_env, ppo_sepsis_optimal_policy, 100))\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean([sum(ep.rewards) for ep in eval_episodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pydef mutable struct GymEnv <: gym.Env\n",
    "    action_space = gym.spaces.Discrete(length(ACTIONS)) # from 0 to 7\n",
    "    observation_space = gym.spaces.Discrete(length(STATES)) # from 0 to length(STATES) - 1\n",
    "    state = rand(0:length(STATES) - 1)\n",
    "    done = false\n",
    "\n",
    "    function __init__(self, transition_model)\n",
    "        self.transition_model = transition_model\n",
    "        self.state = rand(0:length(STATES) - 1)\n",
    "        self.done = false\n",
    "        self.nr_actions = 0\n",
    "    end\n",
    "\n",
    "    function reset(self; seed...)\n",
    "        self.state = rand(0:length(STATES) - 1)\n",
    "        self.done = false\n",
    "        self.nr_actions = 0\n",
    "        return [self.state], Dict()\n",
    "    end\n",
    "\n",
    "    function step(self, action) # action from 0 to 7\n",
    "        transition_probs = self.transition_model[(STATES[self.state + 1], action + 1)]\n",
    "        next_state = categorical(transition_probs) -1\n",
    "        self.nr_actions += 1\n",
    "        reward = get_reward(STATES[self.state + 1])  # Compute reward\n",
    "        self.done = (reward != 0|| self.nr_actions > 100)  # Termination logic\n",
    "        self.state = next_state\n",
    "        return ([self.state], reward, self.done, false, Dict())  # Return Gym-compatible tuple\n",
    "    end\n",
    "\n",
    "    function render(self, mode=\"human\")\n",
    "        println(\"State: $(self.state)\")\n",
    "    end\n",
    "\n",
    "    function close(self)\n",
    "        println(\"Closing environment.\")\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the environment\n",
    "tr = transition_model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <GymEnv object at 0x336bb7ca0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = GymEnv(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Observation: ([2046], Dict{Any, Any}())\n",
      "Obs: [393], Reward: 0.0, Done: false\n",
      "Obs: [1395], Reward: 0.0, Done: false\n",
      "Obs: [2113], Reward: 0.0, Done: false\n",
      "Obs: [1276], Reward: -1.0, Done: true\n",
      "Total Reward: -1.0\n",
      "Closing environment.\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment\n",
    "obs = env.reset()\n",
    "println(\"Initial Observation: $obs\")\n",
    "\n",
    "# Step through the environment\n",
    "done = false\n",
    "total_reward = 0.0\n",
    "\n",
    "while !done\n",
    "    action = env.action_space.sample() # Take a random action\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    println(\"Obs: $obs, Reward: $reward, Done: $done\")\n",
    "end\n",
    "\n",
    "println(\"Total Reward: $total_reward\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running DQN on random transition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <stable_baselines3.dqn.dqn.DQN object at 0x3402c7c70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dqn = sb3.DQN(\"MlpPolicy\", env, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_episodes = []\n",
    "# for _ in 1:100\n",
    "#     opt_pol = get_policy(dqn)\n",
    "#     for _ in 1:1000\n",
    "#         push!(eval_episodes, run_episode(sepsis_env, opt_pol, 100))\n",
    "#     end\n",
    "# end\n",
    "\n",
    "# mean([sum(ep.rewards) for ep in eval_episodes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning from a history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [run_episode(sepsis_env, random_policy(), 100) for _ in 1:1e4];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8853"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_reward = mean([sum(episode.rewards) for episode in history])\n",
    "random_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "function update_state_counts(state_counts::DirichletCounts, episodes::Vector{Episode})\n",
    "    for episode in episodes\n",
    "        for (i, state) in enumerate(episode.visited[1:end-1])\n",
    "            action = episode.policy[state]\n",
    "            new_state = episode.visited[i+1]\n",
    "            state_counts[(state, action, new_state)] = get(state_counts, (state, action, new_state), 1) + 1\n",
    "        end\n",
    "    end\n",
    "    return state_counts\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Tuple{State, Int64}, Vector{Float64}} with 17280 entries:\n",
       "  (State(1, 1, 1, -0.5, tr… => [0.00111337, 0.000788264, 0.000693644, 0.0010753…\n",
       "  (State(-1, -1, 1, 1.0, f… => [0.000149381, 0.000617536, 0.000593287, 0.001233…\n",
       "  (State(-1, -1, -1, 1.0, … => [0.000140698, 0.000716112, 0.000117856, 0.000309…\n",
       "  (State(1, 0, 0, 0.0, fal… => [0.000831603, 4.99648e-6, 0.000776732, 0.0019834…\n",
       "  (State(-1, 1, 0, 0.5, tr… => [0.000511288, 0.00112391, 5.30439e-5, 0.00050322…\n",
       "  (State(-1, 1, -1, 1.0, t… => [0.000921606, 0.000401734, 0.00412987, 1.61151e-…\n",
       "  (State(0, 1, -1, 0.0, tr… => [2.21788e-5, 0.000811698, 0.000562487, 0.0004956…\n",
       "  (State(1, 1, 1, -0.5, tr… => [0.000190075, 0.000369742, 0.000852077, 0.001127…\n",
       "  (State(0, -1, 0, 1.0, tr… => [0.00216023, 0.000416265, 0.000569966, 0.0004093…\n",
       "  (State(0, 0, -1, 0.5, fa… => [3.38045e-5, 0.000694458, 4.25422e-5, 0.00181959…\n",
       "  (State(0, 0, 1, 0.0, fal… => [0.000214809, 0.000550098, 0.000332522, 0.000164…\n",
       "  (State(1, 1, 0, -1.0, tr… => [3.75901e-5, 0.000985973, 0.000192921, 0.0011925…\n",
       "  (State(-1, 0, -1, -0.5, … => [0.000301455, 0.00128599, 0.000232518, 0.0007318…\n",
       "  (State(-1, 1, 1, -1.0, f… => [0.000122281, 0.000276343, 0.00019843, 0.0004189…\n",
       "  (State(-1, 0, 0, -1.0, t… => [0.000337095, 0.00110315, 0.000811153, 0.0002908…\n",
       "  (State(0, 1, -1, 0.0, tr… => [1.57241e-5, 7.94144e-5, 2.4428e-5, 0.000236565,…\n",
       "  (State(-1, -1, 1, -1.0, … => [0.000101597, 0.000170655, 0.000374558, 0.000515…\n",
       "  (State(-1, 0, -1, -0.5, … => [0.00114789, 0.000870593, 0.000800483, 0.0005172…\n",
       "  (State(0, 0, -1, -0.5, f… => [0.000212576, 0.000457779, 0.000406398, 9.53622e…\n",
       "  ⋮                         => ⋮"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state_counts = update_state_counts(DirichletCounts(), history)\n",
    "tr = transition_model(state_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <GymEnv object at 0x3244c1510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = GymEnv(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dqn = sb3.DQN(\"MlpPolicy\", env, verbose=0)\n",
    "# # dqn.learn(total_timesteps=10000, log_interval=1000)\n",
    "# # dqn = sb3.DQN(\"MlpPolicy\", env, replay_buffer_size=10_000, verbose=0)\n",
    "# dqn = sb3.DQN(\"MlpPolicy\", env, batch_size=32, verbose=0)\n",
    "# dqn.learn(total_timesteps=5000, log_interval=500)\n",
    "# policy = get_policy(dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = sb3.PPO(\"MlpPolicy\", env, verbose=0)\n",
    "ppo.learn(total_timesteps=5000)\n",
    "ppo_policy = get_policy(ppo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c = sb3.A2C(\"MlpPolicy\", env, verbose=0)\n",
    "a2c.learn(total_timesteps=5000, log_interval=500)\n",
    "policy = get_policy(a2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [run_episode(sepsis_env, policy, 100) for _ in 1:1000]\n",
    "test_reward = mean([sum(episode.rewards) for episode in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.6",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
