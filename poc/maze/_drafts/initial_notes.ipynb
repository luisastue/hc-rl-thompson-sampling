{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maze environment\n",
    "Check out `maze.jl` to see the custom types and functions that define the belief_maze environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " #   #   .   .   .   .  \n",
      "\n",
      " .   .   G   .   #   #  \n",
      "\n",
      " .   .   .   #   X   #  \n",
      "\n",
      " .   .   #   #   .   .  \n",
      "\n",
      " .   .   .   .   .   .  \n",
      "\n",
      " .   .   .   .   .   .  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant Main.DIRECTIONS. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    }
   ],
   "source": [
    "include(\"maze.jl\")\n",
    "Random.seed!(2)\n",
    "environment, start = generate_deterministic_environment(6)\n",
    "print_maze(environment, start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward function\n",
    "We define the reward function to be -1 for every move in which the agent does not reach the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_reward(pos::Pos, maze::Maze)::Int\n",
    "    if maze[pos] == goal\n",
    "        return 100\n",
    "    else\n",
    "        return -1\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thompson sampling for the maze problem\n",
    "\n",
    "Consists of two tasks:\n",
    "1. a running model of our beliefs about the nature of the problem that becomes more refined as we interact with it.\n",
    "2. a way to find the optimal policy for each fixed value of our beliefs.\n",
    "\n",
    "\n",
    "In each step, we sample from the belief distribution, find the optimal policy for that belief, and execute the policy to get a reward. We then update our belief distribution based on the reward received.\n",
    "\n",
    "\n",
    "(1) In this simple maze problem, what needs to be captured by the belief distribution are the maze and the controller. We do not know what the maze looks like, i.e. where obstacles and the goal are. We do not know the mapping between directions and buttons, so we need to learn that too.\n",
    "\n",
    "\n",
    "(2) Given a maze and a mapping between directions and buttons, we can find an optimal policy using different algorithms. Below are implementations of value iteration, policy iteration and Q-learning for the maze problem. These algorithms are used to find the optimal policy for a fixed belief distribution (controller mapping).\n",
    "\n",
    "### Thompson sampling\n",
    "The agent doesn't know the environment. The environment variable is only used for simulating the agent's interaction with it, but it cannot be used to find the optimal policy. The agent needs to learn the environment by interacting with it.\n",
    "\n",
    "##### From TS tutorial: \n",
    "TS can be applied fruitfully to a broad array of online decision problems beyond the Bernoulli bandit, and we now consider a more general setting. Suppose the agent applies a sequence of actions x1, x2, x3, . . . to a system, selecting each from a set X . This action set could be finite, as in the case of the Bernoulli bandit, or infinite. After applying action xt, the agent observes an outcome yt, which the system randomly generates according to a conditional probability measure qθ(·|xt). The agent enjoys a reward rt = r(yt), where r is a known function. The agent is initially uncertain about the value of θ and represents his uncertainty using a prior distribution p.\n",
    "\n",
    "- action set X: [a, b, c, d]\n",
    "- outcome set Y: Pos(n,n)\n",
    "- probability measure qθ(·|xt): the conditional probability of reaching a position y given action x, given by the environment (maze, current position, controller, movement probabilities).\n",
    "- reward function r(yt): -1 for every move in which the agent does not reach the goal.\n",
    "- prior distribution p: belief distribution over the maze and controller.\n",
    "\n",
    "##### Conditioning\n",
    "- draw a random sample from p\n",
    "- apply action that maximizes expected reward for the model\n",
    "  - expected reward given an action under the current belief: ${\\mathbb{E}_{\\hat{\\theta}}}[r(y_t)|x_t = x] = \\sum_{o} q_{\\hat{\\theta}}(o|x) r(o)$\n",
    "- update p by conditioning on the realized observation ${\\hat{y}_t}$\n",
    "  - conditional distribution: ${\\mathbb{P}_{p,q}(\\theta = u | x_t,y_t) = \\frac{p(u)q_u(y_t|x_t)}{\\sum_{v} p(v)q_v(y_t|x_t)}}$\n",
    "\n",
    "It is important to run a whole episode until a goal is reached, otherwise the reward of an action is too insignificant.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding an optimal policy\n",
    "### Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"value_iteration.jl\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " #↓   #↓   .↓   .↓   .←   .←  \n",
      "\n",
      " .→   .→   G↑   .←   #←   #↑  \n",
      "\n",
      " .↑   .↑   .↑   #↑   X↓   #↓  \n",
      "\n",
      " .↑   .↑   #↑   #↓   .↓   .↓  \n",
      "\n",
      " .↑   .↑   .←   .←   .←   .←  \n",
      "\n",
      " .↑   .↑   .↑   .↑   .↑   .↑  \n",
      "\n",
      " #↓0.0  #↓0.0  .↓100.0  .↓89.0  .←79.0  .←70.0 \n",
      "\n",
      " .→89.0  .→100.0  G↑0.0  .←100.0  #←0.0  #↑0.0 \n",
      "\n",
      " .↑79.0  .↑89.0  .↑100.0  #↑0.0  X↓37.0  #↓0.0 \n",
      "\n",
      " .↑70.0  .↑79.0  #↑0.0  #↓0.0  .↓43.0  .↓37.0 \n",
      "\n",
      " .↑62.0  .↑70.0  .←62.0  .←55.0  .←48.0  .←43.0 \n",
      "\n",
      " .↑55.0  .↑62.0  .↑55.0  .↑48.0  .↑43.0  .↑37.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimal_policy, state_values = value_iteration(environment.maze, environment.controller, 0.9, 0.01)\n",
    "for pos in keys(state_values)\n",
    "    state_values[pos] = round(Int, state_values[pos])  # Use round or Int based on your needs\n",
    "end\n",
    "print_maze(environment, start, optimal_policy, )\n",
    "print_maze(environment, start, optimal_policy, state_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning belief models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gen Functions\n",
    "using Gen\n",
    "\n",
    "@dist function labeled_categorical(labels, probs)\n",
    "    index = categorical(probs)\n",
    "    labels[index]\n",
    "end;\n",
    "\n",
    "@gen function simulate_action(maze::Maze, controller::Controller, pos::Pos, button::Button)::Pos\n",
    "    probs = controller[button]\n",
    "    possible_targets = [get_new_pos(pos, maze, dir) for dir in DIRECTIONS]\n",
    "    target = {button => :new_pos} ~ labeled_categorical(possible_targets, [probs[1], probs[2], probs[3], probs[4]])\n",
    "    return target\n",
    "end;\n",
    "\n",
    "@gen function simulate_episode(maze::Maze, controller::Controller, start::Pos, episode_length::Int, policy::Policy)\n",
    "    maze = environment.maze\n",
    "    pos = start\n",
    "    playing = true\n",
    "    visited = [pos]\n",
    "    rewards = []\n",
    "    for t in 1:episode_length\n",
    "        button = policy[pos]\n",
    "        new_pos = {t => pos} ~ simulate_action(maze, controller, pos, button)\n",
    "        if playing \n",
    "            pos = new_pos\n",
    "            push!(visited, pos)\n",
    "            push!(rewards, get_reward(pos, maze))\n",
    "            if maze[pos] == goal\n",
    "                playing = false\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return Episode(rewards, visited)\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # # . . . .\n",
      " . . G . # #\n",
      " . . . # . #\n",
      " . . # # . .\n",
      " . . . . . .\n",
      " . . . . . .\n"
     ]
    }
   ],
   "source": [
    "trace = simulate(simulate_episode, (environment.maze, environment.controller, start, 100, optimal_policy))\n",
    "choices = get_choices(trace)\n",
    "print_choices(environment, trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function select_controller()::Controller\n",
    "    controller = Controller()\n",
    "    for action in [a,b,c,d]\n",
    "        probs = {action} ~ dirichlet([1.0, 1.0, 1.0, 1.0])\n",
    "        controller[action] = (probs[1], probs[2], probs[3], probs[4])\n",
    "    end\n",
    "    return controller\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Button, NTuple{4, Float64}} with 4 entries:\n",
       "  b => (0.054375, 0.175113, 0.500057, 0.270455)\n",
       "  a => (0.287185, 0.1225, 0.566458, 0.0238573)\n",
       "  c => (0.134206, 0.275522, 0.513829, 0.0764424)\n",
       "  d => (0.469313, 0.0525681, 0.0280788, 0.45004)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_controller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model is not very good because it only has very few parameters and \n",
    "# it will be hard to learn the maze precisely since the categorical distributions leave a lot to chance.\n",
    "\n",
    "@gen function select_maze(n::Int)::Maze\n",
    "    maze = Maze()\n",
    "    goal_x = {:goal_x} ~ uniform_discrete(1, n)\n",
    "    goal_y = {:goal_y} ~ uniform_discrete(1, n)\n",
    "    frac_obstacles = {:frac_obstacles} ~ beta(1,1)\n",
    "    for x in 1:n\n",
    "        for y in 1:n\n",
    "            labels = [obstacle, empty, goal]\n",
    "            pos = Pos(x, y)\n",
    "            if x == goal_x && y == goal_y\n",
    "                maze[pos] = {pos} ~ labeled_categorical(labels, [0.0001, 0.0001, 0.9998])\n",
    "            else\n",
    "                maze[pos] = {pos} ~ labeled_categorical(labels, [frac_obstacles, 1-frac_obstacles-0.0001, 0.0001])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return maze\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # # # # # #\n",
      " # # # # . #\n",
      " # G . # # #\n",
      " . # . . # .\n",
      " # . # # . #\n",
      " X # # # # #\n"
     ]
    }
   ],
   "source": [
    "maze = select_maze(6)\n",
    "print_maze(maze, Pos(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # # . . . .\n",
      " . . G . # #\n",
      " . . . # X #\n",
      " . . # # . .\n",
      " . . . . . .\n",
      " . . . . . .\n"
     ]
    }
   ],
   "source": [
    "Random.seed!(2)\n",
    "environment, start = generate_deterministic_environment(6)\n",
    "print_maze(environment.maze, start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
