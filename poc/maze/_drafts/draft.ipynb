{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Gen;\n",
    "using Random;\n",
    "using StatsBase;\n",
    "using CairoMakie;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@enum Button a=1 b=2 c=3 d=4\n",
    "@enum Direction up=1 down=2 left=3 right=4\n",
    "@enum Field empty=0 obstacle=1 goal=2\n",
    "\n",
    "struct Pos\n",
    "    x::Int\n",
    "    y::Int\n",
    "end\n",
    "\n",
    "const Policy = Dict{Pos, Button}\n",
    "\n",
    "const Maze = Dict{Pos, Field}\n",
    "\n",
    "const Controller = Dict{Direction, Button}\n",
    "\n",
    "struct MovementProbabilities\n",
    "    forward::Float64\n",
    "    back::Float64\n",
    "    left::Float64\n",
    "    right::Float64\n",
    "end;\n",
    "\n",
    "struct Environment\n",
    "    controller::Controller\n",
    "    movementProbabilities::MovementProbabilities\n",
    "    maze::Maze\n",
    "end\n",
    "\n",
    "struct Episode\n",
    "    rewards::Vector{Int}\n",
    "    visited::Vector{Pos}\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "function generate_maze(n::Int)::Maze\n",
    "    maze = Maze()\n",
    "    for x in 1:n\n",
    "        for y in 1:n\n",
    "            maze[Pos(x, y)] = empty\n",
    "        end\n",
    "    end\n",
    "    for i in 1:rand(1:n)\n",
    "        x = rand(1:n)\n",
    "        y = rand(1:n)\n",
    "        maze[Pos(x, y)] = obstacle\n",
    "    end\n",
    "    x = rand(1:n)\n",
    "    y = rand(1:n)\n",
    "    maze[Pos(x, y)] = goal\n",
    "    return maze\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "function generate_environment(n::Int)::Tuple{Environment, Pos}\n",
    "    directions = shuffle([up, down, left, right])\n",
    "    controller = Dict(directions .=> [a, b, c, d])\n",
    "    movementProbs = MovementProbabilities(1, 0, 0, 0)\n",
    "    maze = generate_maze(n) \n",
    "    start = collect(maze)[findfirst(x -> x[2] == empty, collect(maze))][1]\n",
    "    return Environment(controller, movementProbs, maze), start\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Movement in Maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORWARD_MAP = [up, down, left, right]\n",
    "BACK_MAP = [down, up, right, left]\n",
    "LEFT_MAP = [left, right, down, up]\n",
    "RIGHT_MAP = [right, left, up, down]\n",
    "DIRECTION_MAP::Vector{Vector{Direction}} = [FORWARD_MAP, BACK_MAP, LEFT_MAP, RIGHT_MAP]\n",
    "\n",
    "\n",
    "function get_new_pos(pos::Pos, maze::Maze, direction::Direction)\n",
    "    if direction == up\n",
    "        new_pos = Pos(pos.x, pos.y + 1)\n",
    "    elseif direction == down\n",
    "        new_pos = Pos(pos.x, pos.y - 1)\n",
    "    elseif direction == left\n",
    "        new_pos = Pos(pos.x - 1, pos.y)\n",
    "    elseif direction == right\n",
    "        new_pos = Pos(pos.x + 1, pos.y)\n",
    "    end\n",
    "    if haskey(maze, new_pos) && maze[new_pos] != obstacle\n",
    "        return new_pos\n",
    "    else\n",
    "        return pos\n",
    "    end\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dist function stoch_direction(labels, probs)::Direction\n",
    "    index = categorical(probs)\n",
    "    labels[index]\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@gen function simulate_action(pos::Pos, environment::Environment, button)::Pos\n",
    "    direction = collect(environment.controller)[findfirst(x -> x[2] == button, collect(environment.controller))][1]\n",
    "    \n",
    "    prob = environment.movementProbabilities\n",
    "    stoch_dir = {:direction} ~ stoch_direction(DIRECTION_MAP[Int(direction)], [prob.forward, prob.back, prob.left, prob.right])\n",
    "    return get_new_pos(pos, environment.maze, stoch_dir)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_reward(pos::Pos, maze::Maze)::Int\n",
    "    if maze[pos] == goal\n",
    "        return 100\n",
    "    else\n",
    "        return -1\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function maze_model(environment::Environment, start::Pos, episode_length::Int, controller_estimate::Controller, policy::Policy )\n",
    "    maze = environment.maze\n",
    "    pos = start\n",
    "    playing = true\n",
    "    visited = [pos]\n",
    "    rewards = []\n",
    "    for t in 1:episode_length\n",
    "        button = policy[pos]\n",
    "        new_pos = {pos => t} ~ simulate_action(pos, environment, button)\n",
    "        if playing \n",
    "            pos = new_pos\n",
    "            push!(visited, pos)\n",
    "            if maze[pos] == goal\n",
    "                playing = false\n",
    "                push!(rewards, get_reward(pos, maze))\n",
    "                break\n",
    "            else\n",
    "                push!(rewards, get_reward(pos, maze))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return Episode(rewards, visited)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Policy Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function select_policy()::Policy\n",
    "    policy = Policy()\n",
    "    for x in 1:6\n",
    "        for y in 1:6\n",
    "            pos = Pos(x, y)\n",
    "            button = {pos} ~ uniform_discrete(1, 4)\n",
    "            policy[pos] = Button(button)\n",
    "        end\n",
    "    end\n",
    "    return policy\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Controller Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @gen function select_controller()::Controller\n",
    "#     buttons = [a, b, c, d]\n",
    "#     controller = Controller()\n",
    "#     for direction in [up, down, left, right]\n",
    "#         button = {:button => direction} ~ uniform_discrete(1, length(buttons))\n",
    "#         controller[direction] = buttons[button]\n",
    "#         deleteat!(buttons, button)\n",
    "#     end\n",
    "#     return controller\n",
    "# end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(1)\n",
    "environment, start = generate_environment(6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "function print_maze(environment::Environment, start::Pos, policy=Policy(), visit_count=Dict{Pos, Int}())\n",
    "    direction_symbols = Dict(\n",
    "        up => \"↑\", \n",
    "        down => \"↓\", \n",
    "        left => \"←\", \n",
    "        right => \"→\"\n",
    "    )\n",
    "    n = sqrt(length(environment.maze))\n",
    "    for y in reverse(1:n)\n",
    "        row = \"\"\n",
    "        for x in 1:n\n",
    "            pos = Pos(x, y)\n",
    "            if pos == start\n",
    "                row *= \" S\"  # Start\n",
    "            elseif haskey(environment.maze, pos) && environment.maze[pos] == obstacle\n",
    "                row *= \" #\"  # Obstacle\n",
    "            elseif haskey(environment.maze, pos) && environment.maze[pos] == goal\n",
    "                row *= \" G\"  # Goal\n",
    "            elseif haskey(environment.maze, pos) && environment.maze[pos] == empty\n",
    "                row *= \" .\"  \n",
    "            end\n",
    "            if haskey(policy, pos)\n",
    "                direction = collect(environment.controller)[findfirst(x -> x[2] == policy[pos], collect(environment.controller))][1]\n",
    "                row *= direction_symbols[direction] \n",
    "            end\n",
    "            if haskey(visit_count, pos)\n",
    "                row *= string(visit_count[pos]) * \" \" # Print the number of times this position was visited\n",
    "            else\n",
    "                row *= \"  \"  # If no visit count is defined for this position, print an empty space\n",
    "            end\n",
    "        end\n",
    "        println(row)\n",
    "        println()\n",
    "    end\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " .←   .→   .↓   .↓   .↓   .↑  \n",
      "\n",
      " .↓   .↓   .↓   .←   #→   .↓  \n",
      "\n",
      " .←   .→   .↑   .→   S↓   .→  \n",
      "\n",
      " .←   .↓   .→   #↓   .↑   .←  \n",
      "\n",
      " .↑   .↑   .→   .↑   #→   #←  \n",
      "\n",
      " .↑   G↑   .↓   .→   .←   .↓  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_maze(environment, start, select_policy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Pos, Float64} with 36 entries:\n",
       "  Pos(5, 4) => -0.0\n",
       "  Pos(6, 6) => -0.0\n",
       "  Pos(1, 6) => -0.0\n",
       "  Pos(6, 4) => -0.0\n",
       "  Pos(2, 4) => -0.0\n",
       "  Pos(1, 4) => -0.0\n",
       "  Pos(4, 1) => -0.0\n",
       "  Pos(4, 2) => -0.0\n",
       "  Pos(4, 5) => -0.0\n",
       "  Pos(2, 3) => -0.0\n",
       "  Pos(5, 2) => -0.0\n",
       "  Pos(3, 6) => -0.0\n",
       "  Pos(3, 2) => -0.0\n",
       "  Pos(5, 3) => -0.0\n",
       "  Pos(6, 3) => -0.0\n",
       "  Pos(3, 1) => 0.0\n",
       "  Pos(2, 2) => -0.0\n",
       "  Pos(1, 2) => -0.0\n",
       "  Pos(5, 6) => -0.0\n",
       "  Pos(5, 5) => -0.0\n",
       "  Pos(2, 1) => -0.0\n",
       "  Pos(3, 4) => -0.0\n",
       "  Pos(3, 5) => -0.0\n",
       "  Pos(2, 6) => -0.0\n",
       "  Pos(4, 6) => -0.0\n",
       "  ⋮         => ⋮"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function policy_evaluation(policy::Policy, environment::Environment, eps::Float64)\n",
    "    state_values = Dict{Pos, Float64}()\n",
    "    discount = 0.9\n",
    "    for pos in keys(policy)\n",
    "        state_values[pos] = 0\n",
    "    end\n",
    "    changes = [eps + 1]\n",
    "    # while(maximum(changes) > eps)\n",
    "    for i in 1:1e4\n",
    "        changes = []\n",
    "        for pos in keys(policy)\n",
    "            values = []\n",
    "            directions = [up, down, left, right]\n",
    "            new_positions = [get_new_pos(pos, environment.maze, dir) for dir in directions]\n",
    "            for (i, new_pos) in enumerate(new_positions)\n",
    "                new_dir = directions[i]\n",
    "                (weight, actual_pos) = assess(simulate_action, (pos, environment, policy[pos]), choicemap((:direction, new_dir)))\n",
    "                @assert actual_pos == new_pos \"actual position is not as predicted\"\n",
    "                if weight < Inf && weight > -Inf\n",
    "                    retval = get_reward(actual_pos, environment.maze)\n",
    "                    new_value = weight * (discount * state_values[actual_pos] + retval)\n",
    "                    push!(values, new_value)\n",
    "                end\n",
    "            end\n",
    "            new_state_val = sum(values)\n",
    "            push!(changes, abs(new_state_val - state_values[pos]))\n",
    "            state_values[pos] = new_state_val\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return state_values\n",
    "end;\n",
    "\n",
    "policy_evaluation(select_policy(), environment, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Direct utility estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: Very slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " .↓   .↓   .↑   .←   .↑   .↓  \n",
      "\n",
      " .↓   .↓   .↓   .←   #←   .↓  \n",
      "\n",
      " .↓   .↓   .↑   .←   S←   .↓  \n",
      "\n",
      " .←   .↓   .↓   #↑   .↓   .↑  \n",
      "\n",
      " .→   .↑   .→   .↓   #↓   #←  \n",
      "\n",
      " .←   G↑   .↑   .←   .↓   .↓  \n",
      "\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] getindex(A::Vector{Int64}, i1::Int64)",
      "   @ Base ./essentials.jl:13"
     ]
    }
   ],
   "source": [
    "# perform trial from start until terminal state is reached\n",
    "policy = select_policy()\n",
    "print_maze(environment, start, policy, Dict())\n",
    "γ = 0.5\n",
    "state_values = Dict{Pos, Float64}()\n",
    "for pos in keys(policy)\n",
    "    state_values[pos] = 0\n",
    "end\n",
    "visit_count = Dict{Pos, Int}()\n",
    "for i in 1:100000\n",
    "    trace = simulate(maze_model, (environment, start, 100, environment.controller, policy))\n",
    "    episode = get_retval(trace)\n",
    "\n",
    "    for (j, pos) in enumerate(episode.visited)\n",
    "        # println(\"Position \", pos)\n",
    "        if haskey(visit_count, pos)\n",
    "            visit_count[pos] += 1\n",
    "        else\n",
    "            visit_count[pos] = 1\n",
    "        end\n",
    "        state_values[pos] = state_values[pos] + (1 / visit_count[pos]) * (sum(Float64[episode.rewards[k] * (γ^(k-j)) for k in j:length(episode.rewards)]) - state_values[pos]) \n",
    "    end\n",
    "\n",
    "    for pos in keys(policy)\n",
    "        # find the best new position around the currrent position\n",
    "        directions = [up, down, left, right]\n",
    "        new_positions = [get_new_pos(pos, environment.maze, dir) for dir in directions]\n",
    "        new_dir = directions[argmax([state_values[np] for np in new_positions])]\n",
    "        probabilities = []\n",
    "        for button in [a, b, c, d]\n",
    "            (weight, retval) = assess(simulate_action, (pos, environment, button), choicemap((:direction, new_dir)))\n",
    "            push!(probabilities, weight)\n",
    "        end\n",
    "        new_button = Button(argmax(probabilities))\n",
    "        policy[pos] = new_button\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " .↑0.0  .↑0.0  .←-1.958762886597938  .→-1.9591836734693877  .↑0.0  .←-1.9591836734693877 \n",
      "\n",
      " .↑0.0  .↑0.0  .←-2.0  .↑-2.0  #↑0.0  .↑-2.0 \n",
      "\n",
      " .↑0.0  .↑0.0  .↓-2.0  .↓-1.960000805623239  S←-2.0  .↓-2.0 \n",
      "\n",
      " .↑0.0  .↑0.0  .↓0.0  #↓0.0  .→-1.9611650485436904  .↓-1.9595959595959602 \n",
      "\n",
      " .↑0.0  .↑0.0  .↑0.0  .↑0.0  #↓0.0  #↓0.0 \n",
      "\n",
      " .↑0.0  G↑0.0  .↑0.0  .↑0.0  .↑0.0  .↑0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_maze(environment, start, policy, state_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q_learning (generic function with 1 method)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "##### Q-learning Algorithm\n",
    "const QTable = Dict{Pos, Vector{Float64}}\n",
    "\n",
    "function initialize_q_table(maze::Maze)::QTable\n",
    "    q_table = QTable()\n",
    "    for pos in keys(maze)\n",
    "        q_table[pos] = [0.0, 0.0, 0.0, 0.0]  # Four possible directions (up, down, left, right)\n",
    "    end\n",
    "    return q_table\n",
    "end\n",
    "\n",
    "function epsilon_greedy_action(q_values::Vector{Float64}, epsilon::Float64)::Int\n",
    "    if rand() < epsilon\n",
    "        return rand(1:4)  # Explore: Random action\n",
    "    else\n",
    "        return argmax(q_values)  # Exploit: Best action\n",
    "    end\n",
    "end\n",
    "\n",
    "function q_learning(environment::Environment, start::Pos, episodes::Int, alpha::Float64, gamma::Float64, epsilon::Float64)\n",
    "    q_table = initialize_q_table(environment.maze)\n",
    "    \n",
    "    for episode in 1:episodes\n",
    "        pos = start\n",
    "        total_reward = 0\n",
    "        while true\n",
    "            if environment.maze[pos] == goal\n",
    "                break  # Exit when reaching goal\n",
    "            end\n",
    "            \n",
    "            # Choose action using epsilon-greedy policy\n",
    "            action = epsilon_greedy_action(q_table[pos], epsilon)\n",
    "            direction = Direction(action)\n",
    "            \n",
    "            # Get new position based on action\n",
    "            new_pos = get_new_pos(pos, environment.maze, direction)\n",
    "            \n",
    "            # Get reward\n",
    "            reward = get_reward(new_pos, environment.maze)\n",
    "            \n",
    "            # Update Q-value\n",
    "            best_next_q = maximum(q_table[new_pos])\n",
    "            q_table[pos][action] += alpha * (reward + gamma * best_next_q - q_table[pos][action])\n",
    "            \n",
    "            pos = new_pos\n",
    "            total_reward += reward\n",
    "            if goal == environment.maze[new_pos]\n",
    "                break  # Stop if goal reached\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return q_table\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "visualize_q_table (generic function with 1 method)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "function visualize_q_table(environment::Environment, start::Pos, q_table::QTable)\n",
    "    direction_symbols = Dict(\n",
    "        1 => \"↑\", \n",
    "        2 => \"↓\", \n",
    "        3 => \"←\", \n",
    "        4 => \"→\"\n",
    "    )\n",
    "    n = sqrt(length(environment.maze)) |> Int\n",
    "    for y in reverse(1:n)\n",
    "        row = \"\"\n",
    "        for x in 1:n\n",
    "            pos = Pos(x, y)\n",
    "            if pos == start\n",
    "                row *= \" S\"\n",
    "            elseif environment.maze[pos] == obstacle\n",
    "                row *= \" #\"\n",
    "            elseif environment.maze[pos] == goal\n",
    "                row *= \" G\"\n",
    "            else\n",
    "                best_action = argmax(q_table[pos])\n",
    "                row *= \" \" * direction_symbols[best_action]\n",
    "            end\n",
    "        end\n",
    "        println(row)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ← ↓ ↑ ↓ → ↑\n",
      " ↓ ↓ ↓ ↓ # ←\n",
      " → ↓ ← ← S ←\n",
      " → ↓ ← # ↑ ←\n",
      " → ↓ ← → # #\n",
      " → G ↓ ↓ ↑ ↑\n"
     ]
    }
   ],
   "source": [
    "\n",
    "q_table = q_learning(environment, start, 1000, 0.1, 0.9, 0.1)\n",
    "\n",
    "# Visualize the learned policy (Q-table)\n",
    "visualize_q_table(environment, start, q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value_iteration (generic function with 1 method)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function value_iteration(environment::Environment, discount::Float64, eps::Float64)\n",
    "    # Initialize state values for all positions in the maze\n",
    "    state_values = Dict{Pos, Float64}()\n",
    "    for pos in keys(environment.maze)\n",
    "        state_values[pos] = 0.0\n",
    "    end\n",
    "    \n",
    "    policy = Dict{Pos, Button}()\n",
    "    \n",
    "    # Define possible actions\n",
    "    actions = [a, b, c, d]\n",
    "    \n",
    "    # Value iteration loop\n",
    "    delta = eps + 1  # Arbitrary large number to start\n",
    "    while delta > eps\n",
    "        delta = 0\n",
    "        \n",
    "        # Loop through each position in the maze\n",
    "        for pos in keys(environment.maze)\n",
    "            if environment.maze[pos] == goal || environment.maze[pos] == obstacle\n",
    "                continue  # Skip terminal or invalid states\n",
    "            end\n",
    "            \n",
    "            old_value = state_values[pos]\n",
    "            action_values = []\n",
    "            \n",
    "            # Compute expected values for each action\n",
    "            for action in actions\n",
    "                direction = collect(environment.controller)[findfirst(x -> x[2] == action, collect(environment.controller))][1]\n",
    "                new_pos = get_new_pos(pos, environment.maze, direction)\n",
    "                reward = environment.maze[new_pos] == goal ? 100 : -1\n",
    "                action_value = reward + discount * state_values[new_pos]\n",
    "                push!(action_values, action_value)\n",
    "            end\n",
    "            \n",
    "            # Update the state value with the best action's value\n",
    "            state_values[pos] = maximum(action_values)\n",
    "            delta = max(delta, abs(old_value - state_values[pos]))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Extract policy\n",
    "    for pos in keys(environment.maze)\n",
    "        if environment.maze[pos] == goal || environment.maze[pos] == obstacle\n",
    "            continue\n",
    "        end\n",
    "        \n",
    "        action_values = []\n",
    "        for action in actions\n",
    "            direction = collect(environment.controller)[findfirst(x -> x[2] == action, collect(environment.controller))][1]\n",
    "            new_pos = get_new_pos(pos, environment.maze, direction)\n",
    "            reward = environment.maze[new_pos] == goal ? 100 : -1\n",
    "            action_value = reward + discount * state_values[new_pos]\n",
    "            push!(action_values, action_value)\n",
    "        end\n",
    "        \n",
    "        # Select the best action\n",
    "        best_action_idx = argmax(action_values)\n",
    "        policy[pos] = actions[best_action_idx]\n",
    "    end\n",
    "    \n",
    "    return policy, state_values\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " .↓   .↓   .←   .←   .←   .←  \n",
      "\n",
      " .↓   .↓   .←   .←   #   .↓  \n",
      "\n",
      " .↓   .↓   .←   .←   S←   .←  \n",
      "\n",
      " .↓   .↓   .←   #   .↑   .←  \n",
      "\n",
      " .↓   .↓   .←   .←   #   #  \n",
      "\n",
      " .→   G   .←   .←   .←   .←  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimal_policy, state_values = value_iteration(environment, 0.9, 0.01)\n",
    "print_maze(environment, start, optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "policy_iteration (generic function with 1 method)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function policy_iteration(environment::Environment, discount::Float64, eps::Float64)\n",
    "    # Initialize the policy and state values\n",
    "    policy = Dict{Pos, Button}()\n",
    "    state_values = Dict{Pos, Float64}()\n",
    "    for pos in keys(environment.maze)\n",
    "        policy[pos] = a  # Start with some arbitrary action\n",
    "        state_values[pos] = 0.0\n",
    "    end\n",
    "    \n",
    "    stable = false\n",
    "    actions = [a, b, c, d]\n",
    "    \n",
    "    # Policy iteration loop\n",
    "    while !stable\n",
    "        # Policy Evaluation\n",
    "        while true\n",
    "            delta = 0\n",
    "            for pos in keys(environment.maze)\n",
    "                if environment.maze[pos] == goal || environment.maze[pos] == obstacle\n",
    "                    continue  # Skip terminal or invalid states\n",
    "                end\n",
    "                \n",
    "                old_value = state_values[pos]\n",
    "                action = policy[pos]\n",
    "                direction = collect(environment.controller)[findfirst(x -> x[2] == action, collect(environment.controller))][1]\n",
    "                new_pos = get_new_pos(pos, environment.maze, direction)\n",
    "                reward = environment.maze[new_pos] == goal ? 100 : -1\n",
    "                state_values[pos] = reward + discount * state_values[new_pos]\n",
    "                delta = max(delta, abs(old_value - state_values[pos]))\n",
    "            end\n",
    "            \n",
    "            if delta < eps\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # Policy Improvement\n",
    "        stable = true\n",
    "        for pos in keys(environment.maze)\n",
    "            if environment.maze[pos] == goal || environment.maze[pos] == obstacle\n",
    "                continue\n",
    "            end\n",
    "            \n",
    "            old_action = policy[pos]\n",
    "            action_values = []\n",
    "            \n",
    "            for action in actions\n",
    "                direction = collect(environment.controller)[findfirst(x -> x[2] == action, collect(environment.controller))][1]\n",
    "                new_pos = get_new_pos(pos, environment.maze, direction)\n",
    "                reward = environment.maze[new_pos] == goal ? 100 : -1\n",
    "                action_value = reward + discount * state_values[new_pos]\n",
    "                push!(action_values, action_value)\n",
    "            end\n",
    "            \n",
    "            # Choose the best action\n",
    "            best_action_idx = argmax(action_values)\n",
    "            policy[pos] = actions[best_action_idx]\n",
    "            \n",
    "            if policy[pos] != old_action\n",
    "                stable = false  # The policy has changed, so we continue iterating\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return policy, state_values\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " .↓   .↓   .←   .←   .←   .←  \n",
      "\n",
      " .↓   .↓   .←   .←   #←   .↓  \n",
      "\n",
      " .↓   .↓   .←   .←   S←   .←  \n",
      "\n",
      " .↓   .↓   .←   #←   .↑   .←  \n",
      "\n",
      " .↓   .↓   .←   .←   #←   #←  \n",
      "\n",
      " .→   G←   .←   .←   .←   .←  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimal_policy, state_values = policy_iteration(environment, 0.9, 0.01)\n",
    "print_maze(environment, start, optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function condition_on_outcome(trace, pos::Pos, button::Button, new_pos::Pos, reward::Int)::Pos\n",
    "\n",
    "    ## sample maze based on outcome\n",
    "    belief_maze, belief_controller = get_retval(trace)\n",
    "    n = sqrt(length(belief_maze))\n",
    "    direction = belief_controller[button]\n",
    "    labels = [goal, obstacle, empty]\n",
    "    has_goal = false\n",
    "    maze = Maze()\n",
    "    if reward == 100\n",
    "        maze[new_pos] = {:maze => new_pos} ~ labeled_categorical(labels, [0.8, 0.1, 0.1])\n",
    "    elseif new_pos == pos\n",
    "        maze[new_pos] = {:maze => new_pos} ~ labeled_categorical(labels, [0.1, 0.8, 0.1])\n",
    "    else\n",
    "        maze[new_pos] = {:maze => new_pos} ~ labeled_categorical(labels, [0.1, 0.1, 0.8])\n",
    "    end\n",
    "    if maze[new_pos] == goal\n",
    "        has_goal = true\n",
    "    end\n",
    "    \n",
    "    for x in 1:n\n",
    "        for y in 1:n\n",
    "            if x == new_pos.x && y == new_pos.y\n",
    "                continue\n",
    "            end\n",
    "            if has_goal\n",
    "                maze[Pos(x,y)] = {:maze => Pos(x,y)} ~ labeled_categorical(labels, [0.0, 0.15, 0.85])\n",
    "            else\n",
    "                maze[Pos(x,y)] = {:maze => Pos(x,y)} ~ labeled_categorical(labels, [0.1, 0.1, 0.8])\n",
    "            end\n",
    "            if maze[Pos(x, y)] == goal\n",
    "                has_goal = true\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    ## sample controller based on outcome\n",
    "    directions = [up, down, left, right]\n",
    "\n",
    "    direction = new_pos.x - pos.x > 0 ? right : new_pos.x - pos.x < 0 ? left : new_pos.y - pos.y > 0 ? up : down\n",
    "    \n",
    "    controller = Controller()\n",
    "    controller[button] = {:ctrl => button} ~ labeled_categorical([direction], [1.0])\n",
    "    filter!(x -> x != direction, directions)\n",
    "    for b in filter(x -> x != button, [a, b, c, d])\n",
    "        direction = {:ctrl => b} ~ labeled_categorical(directions, [1/length(directions) for _ in directions])\n",
    "        controller[b] = direction\n",
    "        filter!(x -> x != direction, directions)\n",
    "    end\n",
    "\n",
    "    ## sample transitions based on maze, controller and outcome\n",
    "    for k in keys(maze)\n",
    "        for b in keys(controller)\n",
    "            if k == pos  && b == button\n",
    "                target = {k => b} ~ labeled_categorical([new_pos], [1.0])\n",
    "            else\n",
    "                possible_targets = possible_target_pos(k, maze)\n",
    "                target = {k => b} ~ labeled_categorical(possible_targets, fill(1/length(possible_targets), length(possible_targets)))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end;\n",
    "\n",
    "\n",
    "\n",
    "Random.seed!(1)\n",
    "(trace,_) = generate(belief_distribution, (6,))\n",
    "pos = start\n",
    "belief_maze, belief_controller = get_retval(trace)\n",
    "\n",
    "\n",
    "for i in 1:1000\n",
    "    policy, _ = value_iteration(belief_maze, belief_controller, 0.9, 0.01)\n",
    "    # print_maze(environment, pos, policy)\n",
    "    new_pos = simulate_action(pos, environment, policy[pos])\n",
    "    reward = get_reward(new_pos, environment.maze)\n",
    "    println(\"Pos: \", pos, \" Button: \", policy[pos], \" New Pos: \", new_pos, \" Reward: \", reward)\n",
    "\n",
    "    (trace,_) = mh(trace, condition_on_outcome, (pos, policy[pos], new_pos, reward))\n",
    "    belief_maze, belief_controller = get_retval(trace)\n",
    "\n",
    "    println(belief_controller)\n",
    "    print_maze(belief_maze, start)\n",
    "    print_maze(environment, pos, policy)\n",
    "    println(get_choices(trace))\n",
    "    pos = new_pos\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: `@gen` not defined\nin expression starting at In[1]:1",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: `@gen` not defined\nin expression starting at In[1]:1",
      ""
     ]
    }
   ],
   "source": [
    "@gen function simulate_episode(environment::Environment, start::Pos, episode_length::Int, controller_estimate::Controller, policy::Policy )\n",
    "    maze = environment.maze\n",
    "    pos = start\n",
    "    playing = true\n",
    "    visited = [pos]\n",
    "    rewards = []\n",
    "    for t in 1:episode_length\n",
    "        button = policy[pos]\n",
    "        new_pos = {pos => t} ~ simulate_action(pos, environment, button)\n",
    "        if playing \n",
    "            pos = new_pos\n",
    "            push!(visited, pos)\n",
    "            if maze[pos] == goal\n",
    "                playing = false\n",
    "                push!(rewards, get_reward(pos, maze))\n",
    "                break\n",
    "            else\n",
    "                push!(rewards, get_reward(pos, maze))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return Episode(rewards, visited)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function maze_model(as)\n",
    "    T = length(as)\n",
    "\n",
    "    # prior for reward sampling variance\n",
    "    σ ~ exponential(1)\n",
    "\n",
    "    # prior for reward means\n",
    "    q = Vector{Real}(undef, k)\n",
    "    for i in 1:k\n",
    "        q[i] = {(:q, i)} ~ normal(0, 1) # For the moment we assume that the prior for generating random bandits is known.\n",
    "    end\n",
    "\n",
    "    rs = []\n",
    "    for t in 1:T\n",
    "        r = {(:r,t)} ~ normal(q[as[t]], σ) # We also do not assume that the reward variance is known. We infer it from the data.\n",
    "        push!(rs, r)\n",
    "    end\n",
    "    \n",
    "    return rs \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## version where a new policy is learned in each step\n",
    "\n",
    "Random.seed!(2)\n",
    "policies = [random_policy(6)]\n",
    "episodes = choicemap()\n",
    "\n",
    "steps = 10000\n",
    "traces = []\n",
    "for i in 1:10\n",
    "    policy = policies[end]\n",
    "    episode = simulate_episode(environment.maze, environment.controller, start, 100, policy)\n",
    "    for (j, pos) in enumerate(episode.visited)\n",
    "        if j < length(episode.visited)\n",
    "            episodes[i => :episode => pos => j => :new_pos] = episode.visited[j+1]\n",
    "        end\n",
    "    end\n",
    "    trace, _ = generate(maze_model, (policies, start), episodes)\n",
    "    traces = [trace]\n",
    "    for i in 1:steps\n",
    "        trace = block_update(trace)\n",
    "        push!(traces, trace)\n",
    "    end\n",
    "    belief_maze, belief_controller = get_retval(traces[end])\n",
    "    new_policy, _ = value_iteration(belief_maze, belief_controller, 0.9, 0.01)\n",
    "    # print_maze(environment, start, new_policy)\n",
    "    # println()\n",
    "    push!(policies, new_policy)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function get_moves(trace)\n",
    "\n",
    "#     choices = get_choices(trace)\n",
    "#     episodes = nested_view(choices)[:episode]\n",
    "#     moves = Dict()\n",
    "\n",
    "#     for ep_key in collect(keys(episodes))\n",
    "#         episode = episodes[ep_key]\n",
    "#         for step_key in collect(keys(episode))\n",
    "#             step = episode[step_key]\n",
    "#             for pos_key in collect(keys(step))\n",
    "#                 pos = step[pos_key]\n",
    "#                 if !haskey(moves, pos_key)\n",
    "#                     moves[pos_key] = Dict()\n",
    "#                 end\n",
    "#                 for button_key in collect(keys(pos))\n",
    "#                     if haskey(moves[pos_key], button_key)\n",
    "#                         push!(moves[pos_key][button_key], pos[button_key][:new_pos])\n",
    "#                     else \n",
    "#                         moves[pos_key][button_key] = [pos[button_key][:new_pos]]\n",
    "#                     end\n",
    "#                 end\n",
    "#             end\n",
    "#         end\n",
    "#     end\n",
    "\n",
    "#     return moves\n",
    "# end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @gen function propose_field_based_on_moves(trace, pos::Pos, moves::Dict)\n",
    "#     goal_x = get_choices(trace)[:maze => :goal_x]\n",
    "#     goal_y = get_choices(trace)[:maze => :goal_y]\n",
    "#     frac_obstacles = trace[:maze => :frac_obstacles]\n",
    "#     gp = 0.99\n",
    "\n",
    "#     labels = [obstacle, empty, goal]\n",
    "    \n",
    "#     if length(keys(moves)) > 0\n",
    "#         # set new sample for current position\n",
    "#         {:maze => pos} ~ labeled_categorical(labels, [frac_obstacles*gp, (1-frac_obstacles)*gp, 1-gp])\n",
    "#         # and set probable fields for target positions of moves\n",
    "#         for button in keys(moves)\n",
    "#             probs = get_choices(trace)[:ctrl => button]\n",
    "#             dir = DIRECTIONS[argmax(probs)]\n",
    "#             if dir == up\n",
    "#                 target =  Pos(pos.x, pos.y + 1)\n",
    "#             elseif dir == down\n",
    "#                 target =  Pos(pos.x, pos.y - 1)\n",
    "#             elseif dir == left\n",
    "#                 target = Pos(pos.x - 1, pos.y)\n",
    "#             elseif dir == right\n",
    "#                 target = Pos(pos.x + 1, pos.y)\n",
    "#             end\n",
    "#             # if any(new_pos -> new_pos == pos, moves[button]) && has_value(get_choices(trace), :maze => target)\n",
    "#             #     {:maze => target} ~ labeled_categorical(labels, [0.8*gp, 0.2*gp, 1-gp])\n",
    "#             # else\n",
    "#             #     {:maze => target} ~ labeled_categorical(labels, [0.2*gp, 0.8*gp, 1-gp])\n",
    "#             # end\n",
    "#         end\n",
    "#     else\n",
    "#         ## no moves means this is either the goal, or an obstacle\n",
    "#         # if pos.x == goal_x && pos.y == goal_y\n",
    "#         #     new_goal_x, new_goal_y = {*} ~ propose_goal(trace)\n",
    "#         #     println(\"proposed goal: \", new_goal_x, new_goal_y)\n",
    "#         #     if new_goal_x !== pos.x || new_goal_y !== pos.y\n",
    "#         #         new_goal = {:maze => Pos(new_goal_x, new_goal_y)} ~ labeled_categorical([goal], [1.0])\n",
    "#         #         # if there is a new goal, then this should be an obstacle\n",
    "#         #         {:maze => pos} ~ labeled_categorical(labels, [0.8*gp, 0.2*gp, 1-gp])\n",
    "#         #     else\n",
    "#         #         # this still needs to be resampled for the assess in metropolis_hastings\n",
    "#         #         {:maze => pos} ~ labeled_categorical(labels, [0.8*gp, 0.2*gp, 1-gp])\n",
    "#         #     end\n",
    "#         # else\n",
    "#             # not the goal, so it is probably an obstacle\n",
    "#             {:maze => pos} ~ labeled_categorical(labels, [0.8*gp, 0.2*gp, 1-gp])\n",
    "#         # end\n",
    "#     end\n",
    "    \n",
    "\n",
    "# end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @gen function propose_field_based_on_moves(trace, pos::Pos, moves::Dict, new_goal_x, new_goal_y)\n",
    "#     println(\"pos is \", pos)\n",
    "#     goal_x = get_choices(trace)[:maze => :goal_x]\n",
    "#     goal_y = get_choices(trace)[:maze => :goal_y]\n",
    "#     frac_obstacles = trace[:maze => :frac_obstacles]\n",
    "\n",
    "#     labels = [obstacle, empty]\n",
    "#     field_above = Pos(pos.x, pos.y + 1)\n",
    "#     field_below = Pos(pos.x, pos.y - 1)\n",
    "#     field_left  = Pos(pos.x - 1, pos.y)\n",
    "#     field_right = Pos(pos.x + 1, pos.y)\n",
    "#     adj_fields = [field_above, field_below, field_left, field_right]\n",
    "\n",
    "#     if pos.x == new_goal_x && pos.y == new_goal_y\n",
    "#         {:maze => :goal_x} ~ labeled_categorical([new_goal_x], [1.0])\n",
    "#         {:maze => :goal_y} ~ labeled_categorical([new_goal_y], [1.0])\n",
    "#         {:maze => pos} ~ labeled_categorical([goal], [1.0])\n",
    "#         return\n",
    "#     end\n",
    "\n",
    "\n",
    "#     # sample adjacent fields ------------\n",
    "#     for button in keys(moves)\n",
    "#         probs = get_choices(trace)[:ctrl => button]\n",
    "#         dir = argmax(probs)\n",
    "#         target = adj_fields[dir]\n",
    "#         if length(keys(moves)) > 0\n",
    "#             if any(new_pos -> new_pos == pos, moves[button]) && has_value(get_choices(trace), :maze => target)\n",
    "#                 {:maze => target} ~ labeled_categorical(labels, [0.8, 0.2])\n",
    "#             else\n",
    "#                 {:maze => target} ~ labeled_categorical(labels, [0.2, 0.8])\n",
    "#             end\n",
    "#         elseif target.x != new_goal_x || target.y != new_goal_y\n",
    "#             {:maze => target} ~ labeled_categorical(labels, [frac_obstacles, (1-frac_obstacles)])\n",
    "#         end\n",
    "#         println(\"target \", target)\n",
    "#     end\n",
    "\n",
    "#     # sample this field --------------\n",
    "#     if length(keys(moves)) > 0 && !(pos.x == new_goal_x && pos.y == new_goal_y)\n",
    "#             println(\"there are moves \")\n",
    "#             {:maze => pos} ~ labeled_categorical(labels, [frac_obstacles, (1-frac_obstacles)])\n",
    "\n",
    "#         # no moves means this is either the goal or an obstacle\n",
    "#     else \n",
    "#         if pos.x == goal_x && pos.y == goal_y\n",
    "#             println(\"pos is goal \")\n",
    "#             if new_goal_x != pos.x || new_goal_y != pos.y\n",
    "#                 # if there is a new goal, then this should be an obstacle\n",
    "#                 println(\"goal is new \")\n",
    "#                 {:maze => pos} ~ labeled_categorical(labels, [0.8, 0.2])\n",
    "#             end\n",
    "#         elseif !(pos.x == new_goal_x && pos.y == new_goal_y)\n",
    "#             println(\"pos is not goal \")\n",
    "#             # not the goal, so it is probably an obstacle\n",
    "#             {:maze => pos} ~ labeled_categorical(labels, [0.8, 0.2])\n",
    "\n",
    "#         end\n",
    "#     end   \n",
    "\n",
    "# end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function metropolis_hastings_luisa(\n",
    "    trace, proposal::GenerativeFunction, proposal_args::Tuple;\n",
    "    check=false, observations=EmptyChoiceMap())\n",
    "    \n",
    "    model_args = get_args(trace)\n",
    "    argdiffs = map((_) -> NoChange(), model_args)\n",
    "    proposal_args_forward = (trace, proposal_args...,)\n",
    "    (fwd_choices, fwd_weight, _) = propose(proposal, proposal_args_forward)\n",
    "    println(\"fwd_choices\", fwd_choices)\n",
    "    (new_trace, weight, _, discard) = update(trace,\n",
    "        model_args, argdiffs, fwd_choices)\n",
    "        println(\"after update\")\n",
    "    proposal_args_backward = (new_trace, proposal_args...,)\n",
    "    (bwd_weight, _) = assess(proposal, proposal_args_backward, discard)\n",
    "    alpha = weight - fwd_weight + bwd_weight\n",
    "    check && check_observations(get_choices(new_trace), observations)\n",
    "    if log(rand()) < alpha\n",
    "        # accept\n",
    "        return (new_trace, true)\n",
    "    else\n",
    "        # reject\n",
    "        return (trace, false)\n",
    "    end\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function propose_smart_maze(trace, all_moves::Dict)\n",
    "\n",
    "    ## propose_frac_obstacles\n",
    "    \n",
    "    current_value = get_choices(trace)[:maze => :frac_obstacles]\n",
    "    concentration = 10\n",
    "    \n",
    "    # Calculate alpha and beta\n",
    "    α = current_value * concentration\n",
    "    β = (1 - current_value) * concentration\n",
    "    \n",
    "    # Create Beta distribution\n",
    "    frac_obstacles = {:maze => :frac_obstacles} ~ beta(α, β)\n",
    "    \n",
    "        ## Propose goal -------------------------------------------------\n",
    "    \n",
    "        choices = get_choices(trace)\n",
    "        maze_choices = get_submap(choices, :maze)\n",
    "        episodes = nested_view(choices)[:episode]\n",
    "        goal_x = maze_choices[:goal_x]\n",
    "        goal_y = maze_choices[:goal_y]\n",
    "        last_positions = []\n",
    "        for i in collect(keys(episodes))\n",
    "            actions = episodes[i]\n",
    "            max_action = maximum(collect(keys(actions)))\n",
    "            if max_action == 100\n",
    "                continue\n",
    "            else\n",
    "                pos = collect(keys(actions[max_action]))[1]\n",
    "                new_pos = actions[max_action][pos]\n",
    "                action = collect(keys(new_pos))[1]\n",
    "                push!(last_positions, new_pos[action][:new_pos])\n",
    "            end\n",
    "        end\n",
    "        maze = collect(get_values_shallow(maze_choices))\n",
    "        n = Int(sqrt(length(maze) - 3))\n",
    "        xs = [pos.x for pos in last_positions]\n",
    "        x_probs = generate_probabilities(xs, n)\n",
    "        ys = [pos.y for pos in last_positions]\n",
    "        y_probs = generate_probabilities(ys, n)\n",
    "    \n",
    "        new_goal_x = {:maze => :goal_x} ~ categorical(x_probs)\n",
    "        new_goal_y = {:maze => :goal_y} ~ categorical(y_probs)   \n",
    "        {:maze => Pos(new_goal_x, new_goal_y)} ~ labeled_categorical([goal], [1.0])\n",
    "    \n",
    "        labels = [obstacle, empty]\n",
    "        targets = Dict()\n",
    "    \n",
    "        # Propose fields -------------------------------------------------\n",
    "        # for x in 1:n\n",
    "        #     for y in 1:n\n",
    "        #         pos = Pos(x,y)\n",
    "    \n",
    "        #         field_above = Pos(pos.x, pos.y + 1)\n",
    "        #         field_below = Pos(pos.x, pos.y - 1)\n",
    "        #         field_left  = Pos(pos.x - 1, pos.y)\n",
    "        #         field_right = Pos(pos.x + 1, pos.y)\n",
    "        #         adj_fields = [field_above, field_below, field_left, field_right]\n",
    "        #         moves = haskey(all_moves, pos) ? all_moves[pos] : Dict()\n",
    "    \n",
    "        #         # sample adjacent fields ------------\n",
    "        #         for button in keys(moves)\n",
    "        #             probs = get_choices(trace)[:ctrl => button]\n",
    "        #             dir = argmax(probs)\n",
    "        #             target = adj_fields[dir]\n",
    "        #             println(\"target \", target)\n",
    "        #             if haskey(targets, target) || (target.x == new_goal_x && target.y == new_goal_y) ||(target.x == goal_x && target.y == goal_y)\n",
    "        #                 continue\n",
    "        #             end\n",
    "        #             if length(keys(moves)) > 0\n",
    "        #                 if any(new_pos -> new_pos == pos, moves[button]) && has_value(get_choices(trace), :maze => target)\n",
    "        #                     {:maze => target} ~ labeled_categorical(labels, [0.8, 0.2])\n",
    "        #                      targets[target] = true\n",
    "        #                 else\n",
    "        #                     {:maze => target} ~ labeled_categorical(labels, [0.2, 0.8])\n",
    "        #                     targets[target] = true\n",
    "        #                 end\n",
    "        #             else\n",
    "        #                 {:maze => target} ~ labeled_categorical(labels, [frac_obstacles, (1-frac_obstacles)])\n",
    "        #                 targets[target] = true\n",
    "        #             end\n",
    "        #         end\n",
    "        #     end\n",
    "        # end\n",
    "        for x in 1:n\n",
    "            for y in 1:n\n",
    "                pos = Pos(x,y)\n",
    "                # sample this field --------------\n",
    "                if haskey(targets, pos)\n",
    "                    continue\n",
    "                end\n",
    "                if length(keys(moves)) > 0 && !(pos.x == new_goal_x && pos.y == new_goal_y)\n",
    "                        # println(\"there are moves \", pos)\n",
    "                        {:maze => pos} ~ labeled_categorical(labels, [0.2, 0.8])\n",
    "    \n",
    "                    # no moves means this is either the goal or an obstacle\n",
    "                else \n",
    "                    # println(\"no moves \", pos)\n",
    "                    if pos.x == goal_x && pos.y == goal_y\n",
    "                        # println(\"pos is goal \")\n",
    "                        if new_goal_x != pos.x || new_goal_y != pos.y\n",
    "                            # if there is a new goal, then this should be an obstacle\n",
    "                            # println(\"goal is new \")\n",
    "                            {:maze => pos} ~ labeled_categorical(labels, [frac_obstacles, (1-frac_obstacles)])\n",
    "                        end\n",
    "                    elseif !(pos.x == new_goal_x && pos.y == new_goal_y)\n",
    "                        # println(\"pos is not goal \")\n",
    "                        # not the goal, \n",
    "                        {:maze => pos} ~ labeled_categorical(labels, [frac_obstacles, (1-frac_obstacles)])\n",
    "    \n",
    "                    end\n",
    "                end   \n",
    "            end\n",
    "        end\n",
    "    \n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function maze_model_unknown_maze(past_policies::Vector{Policy}, start::Pos)\n",
    "    T = length(past_policies)\n",
    "    @assert T > 0\n",
    "    n = Int(sqrt(length(past_policies[1])))\n",
    "\n",
    "    controller = {:ctrl} ~ select_controller()\n",
    "    maze = {:maze} ~ select_maze(n)\n",
    "\n",
    "    episodes = []\n",
    "    for t in 1:T\n",
    "        policy = past_policies[t]\n",
    "        episode = {:episode => t} ~ simulate_episode(maze, controller, start, 100, policy)\n",
    "        push!(episodes, episode)\n",
    "    end\n",
    "    \n",
    "    return maze, controller\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "function block_update_maze(trace)\n",
    "    for button in [a,b,c,d]\n",
    "        (trace,_) = mh(trace, select(:ctrl => button))\n",
    "    end\n",
    "    \n",
    "    maze_choices = get_submap(get_choices(trace), :maze)\n",
    "    for (pos, field) in collect(get_values_shallow(maze_choices))\n",
    "        (trace,_) = mh(trace, select(:maze => pos))\n",
    "    end\n",
    "\n",
    "    return trace\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(7)\n",
    "policies = [random_policy(6) for i in 1:99]\n",
    "push!(policies, optimal_policy)\n",
    "\n",
    "function generate_episodes(environment, policies)\n",
    "    episodes = choicemap()\n",
    "\n",
    "    for (i, policy) in enumerate(policies)\n",
    "        episode = simulate_episode(environment.maze, environment.controller, start, 100, policy)\n",
    "        for (j, pos) in enumerate(episode.visited)\n",
    "            if j < length(episode.visited)\n",
    "                episodes[:episode => i => j => pos => policy[pos] => :new_pos] = episode.visited[j+1]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return episodes\n",
    "end\n",
    "\n",
    "episodes = generate_episodes(environment, policies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `block_maze_traces` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `block_maze_traces` not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[1]:4"
     ]
    }
   ],
   "source": [
    "using Serialization\n",
    "\n",
    "# Save block_maze_traces to a file\n",
    "serialize(\"block_maze_traces.jls\", block_maze_traces)\n",
    "# Load block_maze_traces from a file\n",
    "# block_maze_traces = deserialize(\"block_maze_traces.jls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using StatsBase  # For the `countmap` function\n",
    "\n",
    "function generate_probabilities(xs, n)\n",
    "    # Step 1: Count occurrences of each number from 1 to n in xs\n",
    "    counts = countmap(xs)  # Returns a dictionary with counts for each number\n",
    "    \n",
    "    # Step 2: Create an array to hold the probabilities\n",
    "    probs = zeros(Float64, n)\n",
    "\n",
    "    # Step 3: Assign probabilities proportional to counts and distance from high-count indices\n",
    "    # Define a decay function that decreases with distance (Gaussian-like decay for example)\n",
    "    function decay(distance, sigma=1.0)\n",
    "        return exp(-distance^2 / (2 * sigma^2))\n",
    "    end\n",
    "    \n",
    "    for i in 1:n\n",
    "        # Base probability is proportional to count\n",
    "        count_prob = haskey(counts, i) && counts[i] > 0 ? counts[i] * 10 : 1\n",
    "        \n",
    "        # Compute distance-based contribution\n",
    "        distance_effect = 0.0\n",
    "        for (j, count) in counts\n",
    "            if count > 0\n",
    "                distance_effect += count * decay(abs(i - j))\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # Combine count-based probability and distance effect\n",
    "        probs[i] = count_prob + distance_effect\n",
    "    end\n",
    "    \n",
    "    # Step 4: Normalize the probabilities so they sum to 1\n",
    "    total = sum(probs)\n",
    "    probs ./= total  # Divide each probability by the total sum to normalize\n",
    "    \n",
    "    return probs\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function propose_frac_obstacles(trace)\n",
    "\n",
    "    current_value = get_choices(trace)[:maze => :frac_obstacles]\n",
    "    concentration = 1\n",
    "\n",
    "    # Calculate alpha and beta\n",
    "    α = current_value * concentration\n",
    "    β = (1 - current_value) * concentration\n",
    "\n",
    "    # Create Beta distribution\n",
    "    frac_obstacles = {:maze => :frac_obstacles} ~ beta(α, β)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function propose_maze(trace)\n",
    "\n",
    "    ## propose goal ---------------------------\n",
    "    choices = get_choices(trace)\n",
    "    maze_choices = get_submap(choices, :maze)\n",
    "    episodes = nested_view(choices)[:episode]\n",
    "    goal_x = maze_choices[:goal_x]\n",
    "    goal_y = maze_choices[:goal_y]\n",
    "    frac_obstacles = choices[:maze => :frac_obstacles]\n",
    "    last_positions = []\n",
    "    for i in collect(keys(episodes))\n",
    "        actions = episodes[i]\n",
    "        max_action = maximum(collect(keys(actions)))\n",
    "        if max_action == 100\n",
    "            continue\n",
    "        else\n",
    "            pos = collect(keys(actions[max_action]))[1]\n",
    "            new_pos = actions[max_action][pos]\n",
    "            action = collect(keys(new_pos))[1]\n",
    "            push!(last_positions, new_pos[action][:new_pos])\n",
    "        end\n",
    "    end\n",
    "    maze = collect(get_values_shallow(maze_choices))\n",
    "    n = Int(sqrt(length(maze) - 3))\n",
    "    xs = [pos.x for pos in last_positions]\n",
    "    x_probs = generate_probabilities(xs, n)\n",
    "    ys = [pos.y for pos in last_positions]\n",
    "    y_probs = generate_probabilities(ys, n)\n",
    "\n",
    "    new_goal_x = {:maze => :goal_x} ~ categorical(x_probs)\n",
    "    new_goal_y = {:maze => :goal_y} ~ categorical(y_probs)   \n",
    "\n",
    "    ## propose fields ---------------------------\n",
    "    labels = [obstacle, empty]\n",
    "    for x in 1:n\n",
    "        for y in 1:n\n",
    "            addr = Pos(x,y)\n",
    "            if x == goal_x && y == goal_y\n",
    "                if new_goal_x !== x || new_goal_y !== y\n",
    "                    new_goal = {:maze => Pos(new_goal_x, new_goal_y)} ~ labeled_categorical([goal], [1.0])\n",
    "                    {:maze => addr} ~ labeled_categorical(labels, [frac_obstacles, 1-frac_obstacles])\n",
    "                end\n",
    "            elseif x != new_goal_x || y != new_goal_y\n",
    "                {:maze => addr} ~ labeled_categorical(labels, [frac_obstacles, 1-frac_obstacles])\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function drift_update_unknown_maze(trace)\n",
    "    for button in [a,b,c,d]\n",
    "        (trace,_) = mh(trace, propose_controller, (button,))\n",
    "    end\n",
    "    (trace,_) = mh(trace, propose_frac_obstacles, ())\n",
    "    \n",
    "    (trace,_) = mh(trace, propose_maze, ())\n",
    "    return trace\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function propose_goal(trace)\n",
    "    choices = get_choices(trace)\n",
    "    maze_choices = get_submap(choices, :maze)\n",
    "    episodes = nested_view(choices)[:episode]\n",
    "    frac_obstacles = nested_view(choices)[:maze => :frac_obstacles]\n",
    "    goal_x = maze_choices[:goal_x]\n",
    "    goal_y = maze_choices[:goal_y]\n",
    "    last_positions = []\n",
    "    for i in collect(keys(episodes))\n",
    "        actions = episodes[i]\n",
    "        max_action = maximum(collect(keys(actions)))\n",
    "        if max_action == 100\n",
    "            continue\n",
    "        else\n",
    "            pos = collect(keys(actions[max_action]))[1]\n",
    "            new_pos = actions[max_action][pos]\n",
    "            action = collect(keys(new_pos))[1]\n",
    "            push!(last_positions, new_pos[action][:new_pos])\n",
    "        end\n",
    "    end\n",
    "    maze = collect(get_values_shallow(maze_choices))\n",
    "    n = Int(sqrt(length(maze) - 3))\n",
    "    xs = [pos.x for pos in last_positions]\n",
    "    x_probs = generate_probabilities(xs, n)\n",
    "    ys = [pos.y for pos in last_positions]\n",
    "    y_probs = generate_probabilities(ys, n)\n",
    "\n",
    "    new_goal_x = {:maze => :goal_x} ~ categorical(x_probs)\n",
    "    new_goal_y = {:maze => :goal_y} ~ categorical(y_probs)   \n",
    "    {:maze => Pos(new_goal_x, new_goal_y)} ~ labeled_categorical([goal], [1.0])\n",
    "    if goal_x != new_goal_x || goal_y != new_goal_y\n",
    "        {:maze => Pos(goal_x, goal_y)} ~ labeled_categorical([obstacle, empty], [frac_obstacles, 1-frac_obstacles])\n",
    "    end\n",
    "\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function propose_field_based_on_moves(trace, pos::Pos, moves::Dict)\n",
    "    # println(\"pos is \", pos)\n",
    "    labels = [obstacle, empty]\n",
    "    field_above = Pos(pos.x, pos.y + 1)\n",
    "    field_below = Pos(pos.x, pos.y - 1)\n",
    "    field_left  = Pos(pos.x - 1, pos.y)\n",
    "    field_right = Pos(pos.x + 1, pos.y)\n",
    "    adj_fields = [field_above, field_below, field_left, field_right]\n",
    "\n",
    "    frac_obstacles = get_choices(trace)[:maze => :frac_obstacles]\n",
    "    targets = Dict()\n",
    "\n",
    "    # sample adjacent fields ------------\n",
    "    for button in keys(moves)\n",
    "        probs = get_choices(trace)[:ctrl => button]\n",
    "        dir = argmax(probs)\n",
    "        target = adj_fields[dir]\n",
    "        targets[target] = true\n",
    "        if length(keys(moves)) > 0\n",
    "            if any(new_pos -> new_pos == target, moves[button])\n",
    "                # println((pos, dir, target, moves[button]))\n",
    "                {:maze => target} ~ labeled_categorical([empty], [1.0])\n",
    "            elseif has_value(get_choices(trace), :maze => target)\n",
    "                {:maze => target} ~ labeled_categorical(labels, [0.8, 0.2])\n",
    "            end\n",
    "        elseif has_value(get_choices(trace), :maze => target)\n",
    "            {:maze => target} ~ labeled_categorical(labels, [0.8, 0.2])\n",
    "        end\n",
    "        # println(\"target \", target)\n",
    "    end\n",
    "\n",
    "    # sample this field --------------\n",
    "    if any(b -> length(moves[b]) > 0, keys(moves))\n",
    "        # println(\"there are moves \")\n",
    "        {:maze => pos} ~ labeled_categorical([empty], [1.0])\n",
    "    else \n",
    "        {:maze => pos} ~ labeled_categorical(labels, [0.8, 0.2])\n",
    "    end   \n",
    "\n",
    "    for field in adj_fields\n",
    "        if !haskey(targets, field) && has_value(get_choices(trace), :maze => field)\n",
    "            {:maze => field} ~ labeled_categorical(labels, [frac_obstacles, (1-frac_obstacles)])\n",
    "        end\n",
    "    end\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_moves(trace)\n",
    "\n",
    "    choices = get_choices(trace)\n",
    "    episodes = nested_view(choices)[:episode]\n",
    "    moves = Dict()\n",
    "\n",
    "    for ep_key in collect(keys(episodes))\n",
    "        episode = episodes[ep_key]\n",
    "        for step_key in collect(keys(episode))\n",
    "            step = episode[step_key]\n",
    "            for pos_key in collect(keys(step))\n",
    "                pos = step[pos_key]\n",
    "                if !haskey(moves, pos_key)\n",
    "                    moves[pos_key] = Dict()\n",
    "                end\n",
    "                for button_key in collect(keys(pos))\n",
    "                    if haskey(moves[pos_key], button_key)\n",
    "                        push!(moves[pos_key][button_key], pos[button_key][:new_pos])\n",
    "                    else \n",
    "                        moves[pos_key][button_key] = [pos[button_key][:new_pos]]\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return moves\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function drift_update_smart_maze(trace, moves::Dict)\n",
    "    for button in [a,b,c,d]\n",
    "        (trace,_) = mh(trace, propose_controller, (button,))\n",
    "    end\n",
    "    \n",
    "    (trace,_) = mh(trace, propose_frac_obstacles, ())\n",
    "\n",
    "    old_goal_x = get_choices(trace)[:maze => :goal_x]\n",
    "    old_goal_y = get_choices(trace)[:maze => :goal_y]\n",
    "\n",
    "    (trace,_) = mh(trace, propose_goal, ())\n",
    "\n",
    "    new_goal_x = get_choices(trace)[:maze => :goal_x]\n",
    "    new_goal_y = get_choices(trace)[:maze => :goal_y]    \n",
    "\n",
    "    maze_choices = get_submap(get_choices(trace), :maze)\n",
    "    for (pos, field) in collect(get_values_shallow(maze_choices))\n",
    "        if pos !== :frac_obstacles && pos !== :goal_x && pos !== :goal_y && pos != Pos(old_goal_x, old_goal_y) && pos != Pos(new_goal_x, new_goal_y)\n",
    "            m = haskey(moves, pos) ? moves[pos] : Dict()\n",
    "            (trace,_) = mh(trace, propose_field_based_on_moves, (pos, m) ) \n",
    "        end\n",
    "    end\n",
    "\n",
    "\n",
    "    return trace\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function interact(environment, trace, smart_maze_traces, episodes, policies)\n",
    "    maze, controller = get_retval(trace)\n",
    "    policy, state_values = value_iteration(maze, controller, 0.9, 0.01)\n",
    "    print_maze(environment, start, policy)\n",
    "\n",
    "    i = length(get_submaps_shallow(get_submap(episodes, :episode))) + 1\n",
    "    episode = simulate_episode(environment.maze, environment.controller, start, 100, policy)\n",
    "    for (j, pos) in enumerate(episode.visited)\n",
    "        if j < length(episode.visited)\n",
    "            episodes[:episode => i => j => pos => policy[pos] => :new_pos] = episode.visited[j+1]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    push!(policies, policy)\n",
    "\n",
    "    trace, _ = generate(maze_model_unknown_maze, (policies, start), episodes)\n",
    "\n",
    "    steps = 20\n",
    "    moves = get_moves(trace)\n",
    "    for i in 1:steps\n",
    "        trace = drift_update_smart_maze(trace, moves)\n",
    "        push!(smart_maze_traces, trace)\n",
    "    end\n",
    "    return trace\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# @gen function select_mazemodel(n::Int, start_pos::Pos, goal_pos::Pos)::MazeModel\n",
    "#     mazemodel = Dict{Pos, Float64}()\n",
    "#     for x in 1:n\n",
    "#         for y in 1:n\n",
    "#             pos = Pos(x, y)\n",
    "#             if x == start_pos.x && y == start_pos.y\n",
    "#                 mazemodel[pos] = {pos} ~ labeled_categorical([Float64(1)], [1]) # because this field is empty if it is the start\n",
    "#             elseif x == goal_pos.x && y == goal_pos.y\n",
    "#                 mazemodel[pos] = {pos} ~ labeled_categorical([Float64(1)], [1]) # because this field is empty if it is the goal\n",
    "#             else\n",
    "#                 mazemodel[pos] = {pos} ~ beta(2,1) \n",
    "#             end\n",
    "#         end\n",
    "#     end\n",
    "#     return mazemodel\n",
    "# end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mazemodel = select_mazemodel(3, Pos(1,1),Pos(3,2))\n",
    "\n",
    "# plot_mazemodel(mazemodel)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
